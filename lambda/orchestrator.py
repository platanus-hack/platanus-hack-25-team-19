import json
import logging
import os
import uuid
import boto3
import requests
import time
from datetime import datetime
from typing import Dict, Any, List

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# --- ANTHROPIC CONFIGURATION ---
# NOTE: In a real environment, the API key would be retrieved from environment variables or a secret manager.
ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY', '')  # Replaced by actual key in runtime environment
MODEL_NAME = "claude-3-opus-20240229"
API_URL = "https://api.anthropic.com/v1/messages"
ANTHROPIC_VERSION = "2023-06-01"
BACKOFF_MAX_RETRIES = 5
BASE_DELAY = 1.0

# Initialize AWS clients
dynamodb = boto3.resource("dynamodb")
sqs = boto3.client("sqs")

# Get environment variables
JOBS_TABLE_NAME = os.environ["JOBS_TABLE_NAME"]
SLACK_QUEUE_URL = os.environ["SLACK_QUEUE_URL"]
MARKET_RESEARCH_QUEUE_URL = os.environ["MARKET_RESEARCH_QUEUE_URL"]
EXTERNAL_RESEARCH_QUEUE_URL = os.environ["EXTERNAL_RESEARCH_QUEUE_URL"]

# Get table reference
jobs_table = dynamodb.Table(JOBS_TABLE_NAME)

# --- SQS MAPPING ---
SQS_MAPPING = {
    "ADI": SLACK_QUEUE_URL,  # Internal Data Audit requires internal contact (Slack)
    "ECG": MARKET_RESEARCH_QUEUE_URL,  # External Context requires Market Research
    "VEE": EXTERNAL_RESEARCH_QUEUE_URL,  # External Experts requires External Research
}


# --- TOOL SCHEMA DEFINITION ---

def get_orchestrator_tool_schema() -> List[Dict[str, Any]]:
    """
    Defines the structured output schema for the Claude model using Anthropic's Tool Use format.
    """
    return [
        {
            "name": "project_quantification_engine_output",
            "description": "The definitive, structured output containing the AI analysis and the full execution details for the ADI, ECG, and VEE research modules. This tool MUST be called to provide the final answer.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "analysis_ia": {
                        "type": "object",
                        "description": "Contains the summary and classification scores of the input problem.",
                        "properties": {
                            "problem_summary": {"type": "string"},
                            "full_problem_declaration": {"type": "string"},
                            "classification_scores": {
                                "type": "object",
                                "properties": {
                                    "internal_data_need": {"type": "integer", "description": "Score 1-5 for ADI need."},
                                    "external_context_need": {"type": "integer", "description": "Score 1-5 for ECG need."},
                                    "specialized_expertise_need": {"type": "integer", "description": "Score 1-5 for VEE need."}
                                }
                            }
                        },
                        "required": ["problem_summary", "full_problem_declaration", "classification_scores"]
                    },
                    "detail_execution": {
                        "type": "object",
                        "description": "Contains the detailed execution payload for the ADI, ECG, and VEE modules. The detailed structure (including 'activate' boolean and specific content) must be generated by the model based on the System Instruction.",
                        "properties": {
                            "ADI": {"type": "object"},
                            "ECG": {"type": "object"},
                            "VEE": {"type": "object"}
                        },
                        "required": ["ADI", "ECG", "VEE"]
                    }
                },
                "required": ["analysis_ia", "detail_execution"]
            }
        }
    ]

# --- SYSTEM INSTRUCTION ---

SYSTEM_INSTRUCTION = (
    "You are the Project Risk Orchestrator (PRO), an AI designed to challenge corporate inertia and validate project ideas. "
    "Your sole purpose is to analyze the completeness of the problem declaration and determine the most efficient, non-redundant research path. "
    "You operate based on three fundamental needs: Internal Data (ADI), External Context (ECG), and Specialized Expertise (VEE). "
    "Analyze the provided FULL_PROBLEM_DECLARATION and the INTERNAL_REGISTRY_DATA."

    "\n\n[1] CLASSIFICATION: Assign a score of 1 (Low) to 5 (High) for the three knowledge needs based ONLY on the input text."
    "\n[2] ACTIVATION LOGIC: Use the following thresholds to set the 'activate' boolean within each module's detail:"
    "\n   - ADI: Activate if the problem is internal, operational, or financial and requires specific company metrics (Score 4 or higher)."
    "\n   - ECG: Activate if the problem involves market trends, competitor behavior, or general industry feasibility (Score 3 or higher)."
    "\n   - VEE: Activate only if the problem is highly technical, legal, or specialized (e.g., Blockchain, AI, new regulation) requiring external validation of viability (Score 5)."

    "\n\n[3] CONTENT GENERATION: For every activated module, generate the full content details as specified in the prompt history:"
    "\n   - ADI: Identify 3-5 specific metrics. Map these metrics to 2-3 specific individuals from the INTERNAL_REGISTRY_DATA whose job or project history makes them the best source. Generate accurate 'justification_specific' fields."
    "\n   - ECG: Generate 3 highly specific search queries and 3 expected quantifiable data points."
    "\n   - VEE: Define the precise expert profile and generate 3 critical, high-level, challenging questions."

    "\n\nYour final output MUST be generated by calling the provided tool."
)


# --- API UTILITY FUNCTION ---

def make_anthropic_api_call_with_backoff(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Handles the POST request to the Anthropic API with exponential backoff."""
    headers = {
        'Content-Type': 'application/json',
        'x-api-key': ANTHROPIC_API_KEY,
        'anthropic-version': ANTHROPIC_VERSION,
        'anthropic-beta': 'tools-2024-05-16' # Required for Tool Use
    }

    for attempt in range(BACKOFF_MAX_RETRIES):
        try:
            response = requests.post(API_URL, headers=headers, data=json.dumps(payload), timeout=60)
            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

            result = response.json()

            # Parse the Anthropic Tool Use response
            for content_block in result.get('content', []):
                if content_block.get('type') == 'tool_use':
                    tool_name = content_block.get('name')
                    tool_args = content_block.get('input', {})

                    if tool_name == "project_quantification_engine_output":
                        return tool_args

            raise Exception("Model did not return the required 'project_quantification_engine_output' via tool use.")

        except requests.exceptions.RequestException as e:
            logger.warning(f"Request Error on attempt {attempt + 1}: {e}. Retrying...")
        except Exception as e:
            logger.error(f"General Error: {e}")
            raise

        if attempt < BACKOFF_MAX_RETRIES - 1:
            delay = BASE_DELAY * (2 ** attempt)
            time.sleep(delay)
        else:
            logger.error("Max retries reached. Failing request.")
            raise Exception("Failed to get response from Anthropic API after max retries.")

    return {}


# --- MAIN HANDLER ---

def handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Orchestrator Lambda function that processes job requests.
    Calls Anthropic to orchestrate the research jobs, persists them to DynamoDB,
    and fans out the job records to the appropriate SQS queues.
    """
    logger.info(f"Received event: {json.dumps(event)}")

    try:
        # 1. Input Parsing and Validation
        body = event.get('body', event)
        if isinstance(body, str):
            body = json.loads(body)

        full_declaration = body.get('full_problem_declaration')
        internal_registry = body.get('internal_registry_data') # Expected string representation of internal data

        if not full_declaration:
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
                'body': json.dumps({'error': 'Missing required parameter: full_problem_declaration'})
            }

        # 2. Construct the User Prompt Content and API Payload
        user_prompt = (
            "Analyze the following project problem declaration and the internal company registry data to generate the complete execution order.\n\n"
            "--- FULL PROBLEM DECLARATION (CONTEXT):\n"
            f"{full_declaration}\n\n"
            "--- INTERNAL REGISTRY DATA (FOR CONTACT MAPPING):\n"
            f"{internal_registry}\n\n"
            "Proceed with the 3-step analysis (Classification, Activation Logic, Content Generation) and return the output STRICTLY in the required JSON schema."
        )

        payload: Dict[str, Any] = {
            "model": MODEL_NAME,
            "system": SYSTEM_INSTRUCTION,
            "max_tokens": 4096,
            "messages": [{"role": "user", "content": user_prompt}],
            "tools": get_orchestrator_tool_schema(),
            "tool_choice": {"type": "tool", "name": "project_quantification_engine_output"}
        }

        # 3. Execute the AI Orchestration
        orchestrated_result = make_anthropic_api_call_with_backoff(payload)

        # 4. Job Persistence (DynamoDB) and SQS Fan-Out Execution
        triggered_jobs = []

        if not orchestrated_result or 'detail_execution' not in orchestrated_result:
            raise Exception("AI orchestration failed to return a valid execution plan.")

        detail_execution = orchestrated_result['detail_execution']
        context_summary = orchestrated_result['analysis_ia']['problem_summary']
        current_time = datetime.utcnow().isoformat()

        # We iterate over the expected keys (ADI, ECG, VEE)
        for tool_key, tool_payload in detail_execution.items():
            if tool_payload.get('activate') is True:
                queue_url = SQS_MAPPING.get(tool_key)

                if queue_url:
                    job_id = str(uuid.uuid4())

                    # A) CREATE JOB RECORD (DynamoDB Persistence)
                    job_item = {
                        'id': job_id,
                        'status': 'CREATED',
                        # Store the full, complex AI instructions as a string
                        'instructions': json.dumps(tool_payload),
                        'context_summary': context_summary,
                        'type': tool_key, # ADI, ECG, or VEE
                        'result': '',
                        'created_at': current_time,
                        'updated_at': current_time
                    }

                    jobs_table.put_item(Item=job_item)
                    logger.info(f"Created job {job_id} ({tool_key}) in DynamoDB")

                    # B) SQS FAN-OUT: Send the job ID and type to the execution queue
                    message_body = {
                        'job_id': job_id,
                        'type': tool_key,
                        # The worker Lambda will retrieve instructions from DynamoDB using the job_id
                    }

                    sqs.send_message(
                        QueueUrl=queue_url,
                        MessageBody=json.dumps(message_body)
                    )
                    logger.info(f"Sent message for job {job_id} to {tool_key} queue")

                    created_job = {
                        'job_id': job_id,
                        'status': 'CREATED',
                        'instructions': tool_payload
                    }
                    triggered_jobs.append(created_job)
                else:
                    logger.warning(f"Tool {tool_key} activated but no SQS URL found in map.")

        # 5. Return the Fan-Out Status
        response = {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({
                'message': 'Orchestration complete. Jobs persisted and fanned out to SQS queues.',
                'jobs': triggered_jobs,
                'classification_scores': orchestrated_result['analysis_ia']['classification_scores']
            })
        }
        logger.info(f"Successfully created {len(triggered_jobs)} jobs")
        return response

    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON in request body: {str(e)}")
        return {
            'statusCode': 400,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': 'Invalid JSON', 'message': str(e)})
        }
    except Exception as e:
        logger.error(f"Error processing request: {str(e)}", exc_info=True)
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': 'Internal server error', 'message': str(e)})
        }
