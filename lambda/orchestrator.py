import json
import logging
import os
import boto3
from datetime import datetime
from typing import Dict, Any, List
from shared.anthropic import Anthropic, ConversationMessage
from shared.job_model import JobModel, JobHandler


def get_organization_registry() -> List[Dict[str, Any]]:
    """Get organization data with fallback for when module is not available."""
    try:
        from shared.organization_diagram import get_organization_data
        return get_organization_data()
    except ImportError:
        logger.warning("Organization diagram module not found, using empty data")
        return []


logger = logging.getLogger()
logger.setLevel(logging.INFO)

# --- ANTHROPIC CONFIGURATION ---
ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY', '')

# Initialize AWS clients
dynamodb = boto3.resource("dynamodb")
sqs = boto3.client("sqs")

# Initialize Anthropic client
anthropic = Anthropic(api_key=ANTHROPIC_API_KEY)

# Get environment variables
JOBS_TABLE_NAME = os.environ["JOBS_TABLE_NAME"]
SLACK_QUEUE_URL = os.environ["SLACK_QUEUE_URL"]
MARKET_RESEARCH_QUEUE_URL = os.environ["MARKET_RESEARCH_QUEUE_URL"]
EXTERNAL_RESEARCH_QUEUE_URL = os.environ["EXTERNAL_RESEARCH_QUEUE_URL"]

# Get table reference
jobs_table = dynamodb.Table(JOBS_TABLE_NAME)

# --- SQS MAPPING ---
SQS_MAPPING = {
    "slack": SLACK_QUEUE_URL,  # Internal Data Audit requires internal contact (Slack)
    "research": MARKET_RESEARCH_QUEUE_URL,  # External Context requires Market Research
    "external_research": EXTERNAL_RESEARCH_QUEUE_URL,  # External Experts requires External Research
}


# --- JOB SCHEMA DEFINITION ---

def get_orchestrator_job_schema():
    """
    Defines the structured output schema for the Claude model using Anthropic's job format.
    """
    return [
        {
            "name": "project_quantification_engine_output",
            "description": "The definitive, structured output containing the AI analysis and the full execution details for the slack, research, and external_research research modules. This job MUST be called to provide the final answer.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "analysis_ia": {
                        "type": "object",
                        "description": "Contains the summary and classification scores of the input problem.",
                        "properties": {
                            "problem_summary": {"type": "string"},
                            "full_problem_declaration": {"type": "string"},
                            "classification_scores": {
                                "type": "object",
                                "properties": {
                                    "internal_data_need": {"type": "integer", "description": "Score 1-5 for slack need."},
                                    "external_context_need": {"type": "integer", "description": "Score 1-5 for research need."},
                                    "specialized_expertise_need": {"type": "integer", "description": "Score 1-5 for external_research need."}
                                }
                            }
                        },
                        "required": ["problem_summary", "full_problem_declaration", "classification_scores"]
                    },
                    "detail_execution": {
                        "type": "object",
                        "description": "Contains the detailed execution payload for the slack, research, and external_research modules. The detailed structure (including 'activate' boolean and specific content) must be generated by the model based on the System Instruction.",
                        "properties": {
                            "slack": {"type": "object"},
                            "research": {"type": "object"},
                            "external_research": {"type": "object"}
                        },
                        "required": ["slack", "research", "external_research"]
                    }
                },
                "required": ["analysis_ia", "detail_execution"]
            }
        }
    ]

# --- SYSTEM INSTRUCTION ---


SYSTEM_INSTRUCTION = '''
You are the Project Risk Orchestrator (PRO), an AI designed to challenge corporate inertia and validate project ideas.
Your sole purpose is to analyze the completeness of the problem declaration and determine the most efficient, non-redundant research path.
You operate based on three fundamental needs: Internal Data (slack), External Context (research), and Specialized Expertise (external_research).
Analyze the provided FULL_PROBLEM_DECLARATION and the INTERNAL_REGISTRY_DATA.

[1] CLASSIFICATION: Assign a score of 1 (Low) to 5 (High) for the three knowledge needs based ONLY on the input text.
[2] ACTIVATION LOGIC: Use the following thresholds to set the 'activate' boolean within each module's detail:
   - slack: Activate if the problem is internal, operational, or financial and requires specific company metrics (Score 3 or higher).
   - research: Activate if the problem involves market trends, competitor behavior, or general industry feasibility (Score 3 or higher).
   - external_research: Activate only if the problem is highly technical, legal, or specialized (e.g., Blockchain, AI, new regulation) requiring external validation not available on our team of viability (Score 3).

[3] CONTENT GENERATION: For every activated module, generate the full content details:
   - slack: Analyze the INTERNAL_REGISTRY_DATA to identify the most relevant people to contact. Select 2-3 specific individuals whose roles, project experience, or expertise align with the problem. Create a 'contacts' array where each contact is a separate object with: name, email, role, justification for selection based on their projects/expertise, and 2-3 specific questions or data points to request from them the questions should be chat answerable, don't require extensive deliverables or studies.
   - research: Generate 3 highly specific search queries and 3 expected quantifiable data points.
   - external_research: Define the precise expert profile and generate 3 critical, high-level, challenging questions.

Your final output MUST be generated by calling the provided job.
When selecting slack contacts, match the problem domain to people's project history and expertise from the INTERNAL_REGISTRY_DATA.
'''


# --- API UTILITY FUNCTION ---

def call_anthropic_with_jobs(system: str, user_prompt: str) -> Dict[str, Any]:
    """Makes a call to the Anthropic API using the shared module with tool execution."""
    try:
        # Create message in the required format
        messages = [ConversationMessage(
            role="user",
            content=user_prompt,
            timestamp=datetime.utcnow().isoformat()
        )]

        # Call with tools support using the unified method
        result = anthropic.send_message(
            messages=messages,
            system=system,
            tools=get_orchestrator_job_schema()
        )

        print("orchestration result:", result)

        return result

    except Exception as e:
        logger.error(f"Error calling Anthropic API: {e}")
        raise
# --- MAIN HANDLER ---


def handler(event: Dict[str, Any], context: Any):
    """
    Orchestrator Lambda function that processes job requests.
    Calls Anthropic to orchestrate the research jobs, persists them to DynamoDB,
    and fans out the job records to the appropriate SQS queues.
    """
    logger.info(f"Received event: {json.dumps(event)}")

    for record in event['Records']:
        try:
            # 1. Input Parsing and Validation
            body = json.loads(record['body'])
            if isinstance(body, str):
                body = json.loads(body)

            full_declaration = body.get('full_problem_declaration')
            session_id = body.get('session_id')

            # Always use organization diagram as internal memory (client onboarding simulation)
            organization_data = get_organization_registry()
            internal_registry = json.dumps(organization_data, indent=2) if organization_data else "No internal registry data available"

            logger.info(f"{session_id} Using organization data for {len(organization_data)} people from internal registry")

            if not full_declaration:
                return {
                    'statusCode': 400,
                    'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
                    'body': json.dumps({'error': 'Missing required parameter: full_problem_declaration'})
                }

            # 2. Construct the User Prompt Content and API Payload
            USER_PROMPT = f'''
                Analyze the following project problem declaration and the internal company registry data to generate the complete execution order.

                --- FULL PROBLEM DECLARATION (CONTEXT):
                {full_declaration}

                --- INTERNAL REGISTRY DATA (FOR CONTACT MAPPING):
                {internal_registry}

                Proceed with the 3-step analysis (Classification, Activation Logic, Content Generation) and return the output STRICTLY in the required JSON schema.
            '''

            # 3. Execute the AI Orchestration
            orchestrated_result = call_anthropic_with_jobs(SYSTEM_INSTRUCTION, USER_PROMPT)

            # 4. Job Persistence (DynamoDB) and SQS Fan-Out Execution
            triggered_jobs = []

            if not orchestrated_result or 'detail_execution' not in orchestrated_result:
                raise Exception("AI orchestration failed to return a valid execution plan.")

            detail_execution = orchestrated_result['detail_execution']
            context_summary = orchestrated_result['analysis_ia']['problem_summary']
            current_time = datetime.utcnow().isoformat()

            # We iterate over the expected keys (slack, research, external_research)
            for job_key, job_payload in detail_execution.items():
                if job_payload.get('activate') is True:
                    queue_url = SQS_MAPPING.get(job_key)

                    if queue_url:
                        # Handle Slack jobs differently - create one job per contact
                        if job_key == 'slack' and 'contacts' in job_payload:
                            for contact in job_payload['contacts']:
                                individual_contact_payload = {
                                    'contact': contact,
                                    'context_summary': context_summary
                                }

                                contact_name_clean = contact['name'].replace(' ', '_').lower()
                                job_item = JobModel(
                                    session_id=session_id,
                                    status='CREATED',
                                    job_type='slack',
                                    instructions=json.dumps(individual_contact_payload),
                                    context_summary=context_summary,
                                    created_at=current_time,
                                    updated_at=current_time
                                )

                                JobHandler(JOBS_TABLE_NAME).create(job=job_item)
                                logger.info(f"Created individual Slack job {job_item.id} for {contact['name']}")

                                # Send to SQS
                                message_body = {
                                    'job_id': job_item.id,
                                    'session_id': session_id
                                }

                                sqs.send_message(
                                    QueueUrl=queue_url,
                                    MessageBody=json.dumps(message_body)
                                )
                                logger.info(f"Sent Slack message for {contact['name']} to queue")

                                triggered_jobs.append({
                                    'job_id': job_item.id,
                                    'status': 'CREATED',
                                    'contact_name': contact['name'],
                                    'job_type': f"slack_{contact_name_clean}"
                                })
                        else:
                            # Handle research and external_research jobs normally (single job per type)
                            job_item = JobModel(
                                session_id=session_id,
                                status='CREATED',
                                job_type=job_key,
                                instructions=json.dumps(job_payload),
                                context_summary=context_summary,
                                created_at=current_time,
                                updated_at=current_time
                            )

                            JobHandler(JOBS_TABLE_NAME).create(job=job_item)
                            logger.info(f"Created job {job_item.id} ({job_key}) in DynamoDB")

                            # Send to SQS
                            message_body = {
                                'job_id': job_item.id,
                                'session_id': session_id
                            }

                            sqs.send_message(
                                QueueUrl=queue_url,
                                MessageBody=json.dumps(message_body)
                            )
                            logger.info(f"Sent message for job {job_item.id} to {job_key} queue")

                            triggered_jobs.append({
                                'job_id': job_item.id,
                                'status': 'CREATED',
                                'job_type': job_key
                            })
                    else:
                        logger.warning(f"Job {job_key} activated but no SQS URL found in map.")

            # 5. Return the Fan-Out Status
            response = {
                'statusCode': 200,
                'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
                'body': json.dumps({
                    'message': 'Orchestration complete. Jobs persisted and fanned out to SQS queues.',
                    'jobs': triggered_jobs,
                    'classification_scores': orchestrated_result['analysis_ia']['classification_scores']
                })
            }
            logger.info(f"Successfully created {len(triggered_jobs)} jobs")
            return response

        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in request body: {str(e)}")
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
                'body': json.dumps({'error': 'Invalid JSON', 'message': str(e)})
            }
        except Exception as e:
            logger.error(f"Error processing request: {str(e)}", exc_info=True)
            return {
                'statusCode': 500,
                'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
                'body': json.dumps({'error': 'Internal server error', 'message': str(e)})
            }
